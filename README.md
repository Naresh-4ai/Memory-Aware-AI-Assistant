---

# ğŸ§  Memory-Aware AI Assistant

A **memory-aware conversational AI** that persistently remembers user interactions using **vector + graph memory**, enabling context-rich, personalized conversations over time.

This project integrates **OpenAI LLMs**, **Mem0**, **Qdrant**, and **Neo4j**, and is fully containerized using **Docker + VS Code Dev Containers** for a reproducible, production-grade development environment.

---

## ğŸš€ What This Project Does

* Accepts continuous user input via CLI
* Retrieves **relevant past memories** before responding
* Injects memory context into the system prompt
* Generates contextual responses using OpenAI
* Stores both **user messages and AI replies** back into memory
* Builds a **long-term conversational memory loop**

This creates an assistant that **does not forget**.

---

## ğŸ§© Architecture Overview

```
User Input
   â†“
Memory Search (Qdrant + Neo4j)
   â†“
Context Injection (System Prompt)
   â†“
OpenAI LLM Response
   â†“
Memory Storage (Vector + Graph)
```

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **OpenAI API**
* **Mem0** (memory orchestration)
* **Qdrant** (vector database)
* **Neo4j** (graph memory)
* **Docker & Docker Compose**
* **VS Code Dev Containers**

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py                # Core memory-aware chat loop
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # App container
â”œâ”€â”€ docker-compose.yaml    # App + Qdrant + Neo4j services
â”œâ”€â”€ devcontainer.json      # VS Code Dev Container config
â”œâ”€â”€ .env                   # Environment variables (not committed)
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone <your-repo-url>
cd <your-repo-name>
```

---

### 2ï¸âƒ£ Create `.env` File

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

> âš ï¸ Never commit `.env` to GitHub

This key is required to initialize the OpenAI client.

---

## ğŸ³ Run with Docker (Recommended)

This project is designed to run as a **multi-service system**.

### Start All Services

```bash
docker compose up --build
```

This will start:

* Python application container
* Qdrant vector database
* Neo4j graph database

---

## ğŸ§© Best Practice: Run Inside VS Code Dev Container â­

For the **cleanest and most reliable experience**, it is **strongly recommended** to run this project inside a **VS Code Dev Container**.

### Why Dev Containers?

* Zero local setup issues
* Correct Python & system dependencies
* Automatic startup of Qdrant & Neo4j
* Identical environment for every developer
* Ideal for serious AI + research projects

### How to Use Dev Container

1. Install:

   * **Docker Desktop**
   * **VS Code**
   * **Dev Containers extension** (Microsoft)

2. Open the project folder in VS Code

3. When prompted, click:

```
Reopen in Container
```

OR manually:

```
Ctrl + Shift + P â†’ Dev Containers: Reopen in Container
```

VS Code will:

* Build containers using `docker-compose.yaml`
* Start all required services
* Attach the editor to the running app container

Once the container is ready, run:

```bash
python main.py
```

âœ… **This is the recommended way to run and develop this project.**

---

## ğŸ§ª Run Locally (Without Docker)

> Use this only if Docker is unavailable.

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Linux / macOS
venv\Scripts\activate     # Windows
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the App

```bash
python main.py
```

---

## ğŸ’¬ How It Works (Internal Flow)

* Loads environment variables
* Initializes:

  * OpenAI client
  * Mem0 memory engine
* For every user message:

  1. Searches relevant memories
  2. Injects memory into system prompt
  3. Generates response via OpenAI
  4. Stores conversation back into memory

This creates **persistent, evolving intelligence**.

---

## ğŸ” Memory Design

* **Vector Memory (Qdrant)** â†’ semantic recall
* **Graph Memory (Neo4j)** â†’ relational understanding
* **Hybrid retrieval** â†’ higher contextual accuracy

---

## ğŸ§  Why This Project Matters

This project explores **next-generation AI agent design**, including:

* Self-updating memory systems
* Context-aware reasoning
* Long-term personalization
* Clear separation of recall, reasoning, and generation

It forms a strong foundation for:

* Autonomous AI agents
* Research-grade memory pipelines
* Reliable long-term assistants
* Personalized AI products

---

## ğŸ”® Future Improvements

* Multi-user memory isolation
* Memory summarization & pruning
* Role-aware memory weighting
* Web / API interface
* LangGraph-based reasoning loops


